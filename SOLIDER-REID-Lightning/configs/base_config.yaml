# Base configuration for SOLIDER-REID Lightning training

# Hydra defaults - allows overriding with model=swin_base, dataset=cctv_reid, etc.
# Note: _self_ first means model/*.yaml and dataset/*.yaml values override base_config
defaults:
  - _self_
  - model: swin_base  # Default model config (can override with model=swin_small, etc.)
  - dataset: cctv_reid  # Default dataset config (can override with dataset=market1501, etc.)

# Model configuration (can be overridden by model/*.yaml)
model:
  name: swin_base_patch4_window7_224  # swin_tiny/small/base, vit_base/small, resnet50
  pretrain_path: ''  # Path to pretrained SOLIDER checkpoint
  semantic_weight: 0.2
  drop_path_rate: 0.1
  drop_rate: 0.0
  attn_drop_rate: 0.0
  neck: bnneck  # bnneck or no
  neck_feat: before  # before or after
  feat_norm: 'yes'  # Normalize features for evaluation

# Dataset configuration
data:
  dataset: market1501  # market1501, msmt17
  root_dir: ./data
  batch_size: 64
  num_instances: 4  # K in PÃ—K sampling for triplet loss
  num_workers: 8
  sampler: softmax_triplet  # softmax, softmax_triplet, id_triplet

  # Image sizes [height, width]
  img_size_train: [384, 128]
  img_size_test: [384, 128]

  # Data augmentation
  augmentation:
    random_flip_prob: 0.5
    random_erase_prob: 0.5
    padding: 10
    pixel_mean: [0.5, 0.5, 0.5]
    pixel_std: [0.5, 0.5, 0.5]

# Loss configuration
loss:
  id_loss_weight: 1.0
  triplet_loss_weight: 1.0
  triplet_margin: 0.3
  label_smoothing: 0.0  # 0.0 for no label smoothing

# Optimizer configuration
optimizer:
  name: SGD  # SGD, Adam, AdamW
  lr: 0.0008
  weight_decay: 0.0001
  momentum: 0.9
  bias_lr_factor: 2.0

# Scheduler configuration
scheduler:
  name: cosine  # cosine, step
  warmup_epochs: 20
  steps: [40, 70]  # For step scheduler
  gamma: 0.1  # For step scheduler

# Training configuration
training:
  max_epochs: 120
  seed: 1234

  # Compute (Lightning 2.6+ format)
  devices: "auto"  # "auto" for auto-detection, or specify number of GPUs
  num_nodes: 1  # Number of nodes for multi-node training
  strategy: "auto"  # "auto" for auto-detection (recommended), "ddp", "fsdp"
  precision: "16-mixed"  # "16-mixed" for AMP, "bf16-mixed" for BFloat16, "32-true" for FP32

  # Optimization
  gradient_clip_val: 0.0  # 0.0 for no gradient clipping
  accumulate_grad_batches: 1

  # Logging & Checkpointing
  log_period: 50  # Log every N steps
  eval_period: 10  # Evaluate every N epochs

  # Resume training
  resume_from: null  # Path to checkpoint to resume from

  # Progress bar
  use_rich_progress: false

# Logging configuration (WandB)
logging:
  wandb_project: solider-reid
  experiment_name: ${model.name}_${data.dataset}

# Output directory
output_dir: ./outputs/${model.name}_${data.dataset}

# PyTorch Lightning ì „ì´ í•™ìŠµ(Fine-tuning) ì‹¤ë¬´ ë§¤ë‰´ì–¼

ì´ ë§¤ë‰´ì–¼ì€ PyTorch Lightningì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ì— í•™ìŠµëœ ëª¨ë¸(Pretrained Model)ì„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ë§ì¶° ì¬í•™ìŠµ(Fine-tuning)í•  ë•Œ ì°¸ì¡°í•˜ê¸° ìœ„í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ëª©ì°¨

1. [ê¸°ë³¸ ì›ì¹™](#1-ê¸°ë³¸-ì›ì¹™-core-principles)
2. [êµ¬í˜„ íŒ¨í„´](#2-êµ¬í˜„-íŒ¨í„´-implementation-patterns)
3. [ì‹¤í–‰ ë° í™œìš©](#3-ì‹¤í–‰-ë°-í™œìš©-execution)
4. [ì£¼ì˜ì‚¬í•­ ë° ì²´í¬í¬ì¸íŠ¸](#4-ì£¼ì˜ì‚¬í•­-ë°-ì²´í¬í¬ì¸íŠ¸-checkpoints)

---

## 1. ê¸°ë³¸ ì›ì¹™ (Core Principles)

### ì™„ë²½í•œ í˜¸í™˜ì„±
- PyTorchì˜ ëª¨ë“  `nn.Module`ì€ Lightningì—ì„œ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ê¸°ì¡´ PyTorch ì½”ë“œë¥¼ Lightningìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë•Œ ëª¨ë¸ êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í‘œì¤€ ì›Œí¬í”Œë¡œìš°
ì „ì´ í•™ìŠµì˜ ì¼ë°˜ì ì¸ ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```
ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë¡œë“œ 
  â†’ íŠ¹ì§• ì¶”ì¶œê¸° ê³ ì •(Freeze) 
  â†’ ìƒˆ ë¶„ë¥˜ê¸° ë¶€ì°© 
  â†’ í•™ìŠµ(Fine-tuning)
```

---

## 2. êµ¬í˜„ íŒ¨í„´ (Implementation Patterns)

ìƒí™©ì— ë§ëŠ” íŒ¨í„´ì„ ì„ íƒí•˜ì—¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

### ğŸ…°ï¸ íŒ¨í„´ A: ê¸°ì¡´ Lightning ì²´í¬í¬ì¸íŠ¸ í™œìš©

**ìƒí™©**: `.ckpt` íŒŒì¼ë¡œ ì €ì¥ëœ Lightning ëª¨ë¸ì˜ ì¼ë¶€ë¥¼ ê°€ì ¸ì™€ ì‚¬ìš©í•  ë•Œ

```python
import torch.nn as nn
from pytorch_lightning import LightningModule

class MyTransferModel(LightningModule):
    def __init__(self, checkpoint_path, num_classes=10):
        super().__init__()
        
        # 1. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ
        # (ì£¼ì˜: ì „ì²´ ëª¨ë¸ì´ ë¡œë“œë˜ë¯€ë¡œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œí•´ì„œ í• ë‹¹í•´ì•¼ í•¨)
        pretrained_model = AutoEncoder.load_from_checkpoint(checkpoint_path)
        self.feature_extractor = pretrained_model.encoder 
        
        # 2. ê°€ì¤‘ì¹˜ ê³ ì • (Freeze)
        # ê¸°ì¡´ ëª¨ë¸ì˜ ì§€ì‹ì´ íŒŒê´´ë˜ì§€ ì•Šë„ë¡ ì—…ë°ì´íŠ¸ë¥¼ ë§‰ìŠµë‹ˆë‹¤.
        self.feature_extractor.freeze()
        
        # 3. ìƒˆë¡œìš´ ì‘ì—…ì— ë§ëŠ” ë¶„ë¥˜ê¸°(Head) ë¶€ì°©
        # ì˜ˆ: Encoder ì¶œë ¥(100 dim) -> ë‚´ ë°ì´í„° í´ë˜ìŠ¤(10ê°œ)
        self.classifier = nn.Linear(100, num_classes)

    def forward(self, x):
        # 4. íŠ¹ì§• ì¶”ì¶œ (Gradient ê³„ì‚° ì•ˆ ë¨ - freeze íš¨ê³¼)
        representations = self.feature_extractor(x)
        
        # 5. ë¶„ë¥˜ (ì´ ë¶€ë¶„ë§Œ í•™ìŠµë¨)
        x = self.classifier(representations)
        return x
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- `load_from_checkpoint()`ë¡œ ì „ì²´ ëª¨ë¸ì„ ë¡œë“œí•œ í›„ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
- `freeze()`ë¡œ íŠ¹ì§• ì¶”ì¶œê¸°ì˜ ê°€ì¤‘ì¹˜ ê³ ì •
- ìƒˆë¡œìš´ ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •

---

### ğŸ…±ï¸ íŒ¨í„´ B: ì™¸ë¶€ Vision ëª¨ë¸ í™œìš© (Torchvision)

**ìƒí™©**: ResNet, EfficientNet ë“± ImageNetìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ Feature Extractorë¡œ ì‚¬ìš©í•  ë•Œ

```python
import torch
import torch.nn as nn
import torchvision.models as models
from pytorch_lightning import LightningModule

class VisionFinetuner(LightningModule):
    def __init__(self, num_classes):
        super().__init__()

        # 1. Backbone ë¡œë“œ (weights="DEFAULT"ëŠ” ìµœì‹  ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        backbone = models.resnet50(weights="DEFAULT")
        
        # 2. ëª¨ë¸ ìˆ˜ìˆ  (ë§ˆì§€ë§‰ FC ë ˆì´ì–´ ì œê±°)
        # ResNetì€ ë§ˆì§€ë§‰ì´ 'fc'ì´ë¯€ë¡œ ì´ë¥¼ ì œì™¸í•œ ëª¨ë“  ë ˆì´ì–´ë¥¼ ì‚¬ìš©
        layers = list(backbone.children())[:-1]
        self.feature_extractor = nn.Sequential(*layers)
        
        # 3. ê°€ì¤‘ì¹˜ ê³ ì • ë° í‰ê°€ ëª¨ë“œ ì„¤ì • (ì¤‘ìš”!)
        # BatchNorm ë“±ì´ í•™ìŠµ ëª¨ë“œë¡œ ë™ì‘í•˜ì—¬ í†µê³„ê°€ í‹€ì–´ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ eval() í•„ìˆ˜
        self.feature_extractor.eval()
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

        # 4. ìƒˆ ë¶„ë¥˜ê¸° ë¶€ì°©
        num_filters = backbone.fc.in_features  # ResNet50 = 2048
        self.classifier = nn.Linear(num_filters, num_classes)

    def forward(self, x):
        # 5. ì¶”ë¡  ëª¨ë“œë¡œ íŠ¹ì§• ì¶”ì¶œ (no_grad í•„ìˆ˜)
        with torch.no_grad():
            representations = self.feature_extractor(x).flatten(1)
            
        # 6. ë¶„ë¥˜
        x = self.classifier(representations)
        return x
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- `weights="DEFAULT"`ë¡œ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ìë™ ë‹¤ìš´ë¡œë“œ
- ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ ì œê±° í›„ ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ë¶€ì°©
- **ë°˜ë“œì‹œ `eval()` ëª¨ë“œ ì„¤ì •** - BatchNorm, Dropout ë¹„í™œì„±í™”
- `torch.no_grad()`ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ

---

### Â©ï¸ íŒ¨í„´ C: ì™¸ë¶€ NLP ëª¨ë¸ í™œìš© (Hugging Face)

**ìƒí™©**: BERT, GPT ë“± íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í™œìš©í•  ë•Œ

```python
import torch.nn as nn
from transformers import BertModel
from pytorch_lightning import LightningModule

class BertFinetuner(LightningModule):
    def __init__(self, num_classes):
        super().__init__()

        # 1. Hugging Face ëª¨ë¸ ë¡œë“œ
        self.bert = BertModel.from_pretrained("bert-base-cased")
        
        # 2. ëª¨ë“œ ì„¤ì •
        # ì „ì²´ë¥¼ ë¯¸ì„¸ì¡°ì •(Fine-tuning) í•˜ë ¤ë©´ train() ëª¨ë“œ ìœ ì§€
        # Featureë¡œë§Œ ì“´ë‹¤ë©´ eval() ë° íŒŒë¼ë¯¸í„° freeze() í•„ìš”
        self.bert.train() 
        
        # 3. ë¶„ë¥˜ê¸° ë¶€ì°©
        self.W = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask, token_type_ids):
        # 4. BERT í†µê³¼
        outputs = self.bert(input_ids=input_ids, 
                            attention_mask=attention_mask, 
                            token_type_ids=token_type_ids)
        
        # 5. CLS í† í°(ë¬¸ì¥ ì „ì²´ ì˜ë¯¸) ì¶”ì¶œ
        h_cls = outputs.last_hidden_state[:, 0]
        
        # 6. ë¶„ë¥˜
        logits = self.W(h_cls)
        return logits
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- `from_pretrained()`ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
- Fine-tuning vs Feature Extractionì— ë”°ë¼ ëª¨ë“œ ì„ íƒ
- CLS í† í°ì„ í™œìš©í•œ ë¬¸ì¥ ë¶„ë¥˜

---

## 3. ì‹¤í–‰ ë° í™œìš© (Execution)

### í•™ìŠµ (Training)

```python
from pytorch_lightning import Trainer

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = VisionFinetuner(num_classes=10)

# Trainer ì„¤ì • ë° í•™ìŠµ
trainer = Trainer(
    max_epochs=10,
    accelerator="gpu",
    devices=1,
    precision=16,  # Mixed precision training (ì„ íƒì‚¬í•­)
)

trainer.fit(
    model, 
    train_dataloaders=train_loader, 
    val_dataloaders=val_loader
)
```

### ì¶”ë¡  (Inference)

```python
# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
model = VisionFinetuner.load_from_checkpoint("best_model.ckpt")
model.freeze()  # ì¶”ë¡  ì‹œì—ëŠ” ë°˜ë“œì‹œ freeze (ë©”ëª¨ë¦¬ ì ˆì•½)
model.eval()

# ë°ì´í„° ì˜ˆì¸¡
with torch.no_grad():
    predictions = model(input_data)
```

---

## 4. ì£¼ì˜ì‚¬í•­ ë° ì²´í¬í¬ì¸íŠ¸ (Checkpoints)

### ğŸ’¡ Freeze vs Train ì „ëµ

#### ë°ì´í„°ê°€ ì ì„ ë•Œ (Small Dataset)
- **ì „ëµ**: Feature Extractorë¥¼ `freeze()` í•˜ê³  Classifierë§Œ í•™ìŠµ
- **ì´ìœ **: Overfitting ë°©ì§€
- **í•™ìŠµë¥ **: Classifierì— ëŒ€í•´ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ í•™ìŠµë¥  ì‚¬ìš© (ì˜ˆ: 1e-3)

```python
# Feature extractor ê³ ì •
for param in self.feature_extractor.parameters():
    param.requires_grad = False

# Classifierë§Œ í•™ìŠµ
optimizer = torch.optim.Adam(self.classifier.parameters(), lr=1e-3)
```

#### ë°ì´í„°ê°€ ë§ì„ ë•Œ (Large Dataset)
- **ì „ëµ**: ì „ì²´ ëª¨ë¸ì„ `unfreeze` í•˜ë˜, Learning Rateë¥¼ ë§¤ìš° ë‚®ê²Œ ì„¤ì •
- **ì´ìœ **: ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì²œì²œíˆ ì¡°ì •
- **í•™ìŠµë¥ **: 
  - Feature Extractor: ë§¤ìš° ë‚®ì€ í•™ìŠµë¥  (1e-4 ~ 1e-5)
  - Classifier: ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ í•™ìŠµë¥  (1e-3)

```python
# ì°¨ë³„í™”ëœ í•™ìŠµë¥  ì„¤ì •
optimizer = torch.optim.Adam([
    {'params': self.feature_extractor.parameters(), 'lr': 1e-5},
    {'params': self.classifier.parameters(), 'lr': 1e-3}
])
```

### âš ï¸ Eval Modeì˜ ì¤‘ìš”ì„± (Vision)

**ë¬¸ì œ**: Feature Extractorë¥¼ ê³ ì •í•´ì„œ ì‚¬ìš©í•  ë•Œ `eval()` ëª¨ë“œë¥¼ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´:
- BatchNormì´ í•™ìŠµ ëª¨ë“œë¡œ ë™ì‘í•˜ì—¬ í†µê³„ê°€ ê³„ì† ì—…ë°ì´íŠ¸ë¨
- Dropoutì´ í™œì„±í™”ë˜ì–´ ì¼ê´€ì„± ì—†ëŠ” íŠ¹ì§• ì¶”ì¶œ
- **ê²°ê³¼**: ì„±ëŠ¥ ì €í•˜ ë° ë¶ˆì•ˆì •í•œ í•™ìŠµ

**í•´ê²°ì±…**:
```python
# ë°˜ë“œì‹œ eval() ëª¨ë“œ ì„¤ì •
self.feature_extractor.eval()

# ë˜ëŠ” freeze()ì™€ í•¨ê»˜ ì‚¬ìš©
self.feature_extractor.freeze()  # Lightningì˜ freeze()ëŠ” ìë™ìœ¼ë¡œ eval()ë„ ì„¤ì •
```

### ğŸ“ ì…ë ¥ í¬ê¸° (Input Size)

**ì¤‘ìš”**: ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ í¬ê¸°ì™€ ì •ê·œí™” ë°©ì‹ì„ ì •í™•íˆ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.

#### Vision ëª¨ë¸ ì˜ˆì‹œ:
```python
from torchvision import transforms

# ImageNet ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ ê²½ìš°
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),  # ResNet, EfficientNet ë“±ì€ 224x224
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # ImageNet í†µê³„
        std=[0.229, 0.224, 0.225]
    )
])
```

#### NLP ëª¨ë¸ ì˜ˆì‹œ:
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
# ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” max_length í™•ì¸ (ì¼ë°˜ì ìœ¼ë¡œ 512)
encoded = tokenizer(text, max_length=512, padding=True, truncation=True)
```

### ğŸ”§ ì¶”ê°€ ìµœì í™” íŒ

1. **Learning Rate Scheduler ì‚¬ìš©**
   ```python
   scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
   ```

2. **Early Stopping**
   ```python
   from pytorch_lightning.callbacks import EarlyStopping
   
   early_stop = EarlyStopping(monitor="val_loss", patience=3)
   trainer = Trainer(callbacks=[early_stop])
   ```

3. **Gradient Clipping**
   ```python
   trainer = Trainer(gradient_clip_val=1.0)
   ```

4. **Mixed Precision Training**
   ```python
   trainer = Trainer(precision=16)  # FP16 ì‚¬ìš©
   ```

---

## ìš”ì•½ (Summary)

| í•­ëª© | ê¶Œì¥ ì‚¬í•­ |
|------|----------|
| **ì†Œê·œëª¨ ë°ì´í„°** | Feature Extractor ê³ ì •, Classifierë§Œ í•™ìŠµ |
| **ëŒ€ê·œëª¨ ë°ì´í„°** | ì „ì²´ ëª¨ë¸ í•™ìŠµ, ì°¨ë³„í™”ëœ í•™ìŠµë¥  ì ìš© |
| **Vision ëª¨ë¸** | ë°˜ë“œì‹œ `eval()` ëª¨ë“œ ì„¤ì • |
| **ì…ë ¥ í¬ê¸°** | ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ ìš”êµ¬ì‚¬í•­ ì •í™•íˆ ì¤€ìˆ˜ |
| **ì •ê·œí™”** | ì‚¬ì „ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ í†µê³„ê°’ ì‚¬ìš© |

---

## ì°¸ê³  ìë£Œ

- [PyTorch Lightning ê³µì‹ ë¬¸ì„œ](https://lightning.ai/docs/pytorch/stable/)
- [Torchvision Models](https://pytorch.org/vision/stable/models.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)


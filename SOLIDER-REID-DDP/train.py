#!/usr/bin/env python
"""
Native PyTorch DDP Training Script for SOLIDER-REID

Features:
- Native PyTorch Distributed Data Parallel (DDP)
- SLURM-aware distributed training
- Mixed precision training with torch.amp
- WandB logging
- Checkpointing with best model tracking
"""

import os
import sys
import argparse
import time
from pathlib import Path

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.amp import autocast, GradScaler
import torchvision.transforms as T
import numpy as np
import yaml
from types import SimpleNamespace

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from models.reid_model import ReIDModel
from data.bases import ImageDataset
from data.cctv_reid import CCTVReID
from data.market1501 import Market1501
from data.sampler_ddp import RandomIdentitySampler_DDP
from data.transforms import RandomErasing
from losses.triplet_loss import TripletLoss
from losses.softmax_loss import LabelSmoothingCrossEntropy
from metrics.reid_metrics import evaluate
from utils.distributed import (
    init_distributed, is_main_process, get_rank, get_world_size,
    reduce_tensor, all_gather_tensor, cleanup
)
from utils.logger import setup_logger
from utils.meter import AverageMeter, MetricMeter


def load_config(config_path: str) -> SimpleNamespace:
    """Load YAML configuration"""
    with open(config_path, 'r') as f:
        cfg_dict = yaml.safe_load(f)

    def dict_to_namespace(d):
        if isinstance(d, dict):
            return SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})
        elif isinstance(d, list):
            return [dict_to_namespace(i) for i in d]
        return d

    return dict_to_namespace(cfg_dict)


def build_transforms(cfg, is_train: bool = True):
    """Build image transforms"""
    if is_train:
        img_size = cfg.data.img_size_train
    else:
        img_size = cfg.data.img_size_test

    normalize = T.Normalize(
        mean=cfg.data.augmentation.pixel_mean,
        std=cfg.data.augmentation.pixel_std
    )

    if is_train:
        transform = T.Compose([
            T.Resize(img_size),
            T.RandomHorizontalFlip(p=cfg.data.augmentation.random_flip_prob),
            T.Pad(cfg.data.augmentation.padding),
            T.RandomCrop(img_size),
            T.ToTensor(),
            normalize,
            RandomErasing(probability=cfg.data.augmentation.random_erase_prob),
        ])
    else:
        transform = T.Compose([
            T.Resize(img_size),
            T.ToTensor(),
            normalize,
        ])

    return transform


def build_dataloader(cfg, is_train: bool = True):
    """Build data loaders for training and validation"""
    # Build dataset
    if cfg.data.dataset == 'cctv_reid':
        dataset = CCTVReID(root=cfg.data.root_dir)
    elif cfg.data.dataset == 'market1501':
        dataset = Market1501(root=cfg.data.root_dir)
    else:
        raise ValueError(f"Unknown dataset: {cfg.data.dataset}")

    # Build transforms
    train_transform = build_transforms(cfg, is_train=True)
    val_transform = build_transforms(cfg, is_train=False)

    if is_train:
        # Training dataloader with identity sampler
        train_set = ImageDataset(dataset.train, transform=train_transform)

        # Use RandomIdentitySampler for PÃ—K sampling
        sampler = RandomIdentitySampler_DDP(
            dataset.train,
            batch_size=cfg.data.batch_size,
            num_instances=cfg.data.num_instances
        )

        train_loader = DataLoader(
            train_set,
            batch_size=cfg.data.batch_size // get_world_size(),
            sampler=sampler,
            num_workers=cfg.data.num_workers,
            pin_memory=True,
            drop_last=True,
        )

        # Validation dataloader (query + gallery)
        val_set = ImageDataset(
            dataset.query + dataset.gallery,
            transform=val_transform
        )

        val_sampler = DistributedSampler(val_set, shuffle=False) if get_world_size() > 1 else None

        val_loader = DataLoader(
            val_set,
            batch_size=cfg.data.batch_size,
            sampler=val_sampler,
            num_workers=cfg.data.num_workers,
            pin_memory=True,
            shuffle=False,
        )

        return train_loader, val_loader, dataset.num_train_pids, len(dataset.query)
    else:
        # Test only
        val_set = ImageDataset(
            dataset.query + dataset.gallery,
            transform=val_transform
        )

        val_sampler = DistributedSampler(val_set, shuffle=False) if get_world_size() > 1 else None

        val_loader = DataLoader(
            val_set,
            batch_size=cfg.data.batch_size,
            sampler=val_sampler,
            num_workers=cfg.data.num_workers,
            pin_memory=True,
            shuffle=False,
        )

        return val_loader, len(dataset.query)


def build_optimizer(cfg, model):
    """Build optimizer with per-parameter options"""
    params = []
    for key, value in model.named_parameters():
        if not value.requires_grad:
            continue

        lr = cfg.optimizer.lr
        weight_decay = cfg.optimizer.weight_decay

        # Different LR for bias
        if "bias" in key:
            lr = cfg.optimizer.lr * cfg.optimizer.bias_lr_factor
            weight_decay = 0

        params.append({
            "params": [value],
            "lr": lr,
            "weight_decay": weight_decay
        })

    if cfg.optimizer.name == 'SGD':
        optimizer = torch.optim.SGD(params, momentum=cfg.optimizer.momentum)
    elif cfg.optimizer.name == 'Adam':
        optimizer = torch.optim.Adam(params)
    elif cfg.optimizer.name == 'AdamW':
        optimizer = torch.optim.AdamW(params)
    else:
        raise ValueError(f"Unknown optimizer: {cfg.optimizer.name}")

    return optimizer


def build_scheduler(cfg, optimizer):
    """Build learning rate scheduler"""
    if cfg.scheduler.name == 'cosine':
        # Warmup + Cosine
        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=0.01,
            total_iters=cfg.scheduler.warmup_epochs
        )
        main_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=cfg.training.max_epochs - cfg.scheduler.warmup_epochs,
            eta_min=1e-7
        )
        scheduler = torch.optim.lr_scheduler.SequentialLR(
            optimizer,
            schedulers=[warmup_scheduler, main_scheduler],
            milestones=[cfg.scheduler.warmup_epochs]
        )
    elif cfg.scheduler.name == 'step':
        scheduler = torch.optim.lr_scheduler.MultiStepLR(
            optimizer,
            milestones=cfg.scheduler.steps,
            gamma=cfg.scheduler.gamma
        )
    else:
        raise ValueError(f"Unknown scheduler: {cfg.scheduler.name}")

    return scheduler


def train_one_epoch(
    model, train_loader, optimizer, scheduler,
    ce_loss_fn, triplet_loss_fn, scaler, cfg, epoch, logger, global_step=0
):
    """Train for one epoch"""
    model.train()

    meters = MetricMeter()
    batch_time = AverageMeter('Time', ':6.3f')
    data_time = AverageMeter('Data', ':6.3f')

    num_batches = len(train_loader)
    end = time.time()

    for batch_idx, batch in enumerate(train_loader):
        global_step += 1
        data_time.update(time.time() - end)

        # Move data to GPU
        imgs, pids, camids, trackids, _ = batch
        imgs = imgs.cuda(non_blocking=True)
        pids = pids.cuda(non_blocking=True)

        # Forward pass with mixed precision
        with autocast(device_type='cuda', dtype=torch.float16, enabled=cfg.training.use_amp):
            cls_score, global_feat, _ = model(imgs, label=pids)

            # Compute losses
            id_loss = ce_loss_fn(cls_score, pids)
            triplet_loss, dist_ap, dist_an = triplet_loss_fn(global_feat, pids)

            # Handle edge case: no valid triplets
            if dist_ap is None or dist_an is None:
                triplet_loss = torch.tensor(0.0, device=imgs.device, requires_grad=True)
                triplet_prec = torch.tensor(0.0, device=imgs.device)
            else:
                if dist_ap.sum() > 0:
                    triplet_prec = (dist_an > dist_ap).float().mean()
                else:
                    triplet_prec = torch.tensor(0.0, device=imgs.device)

            # Combined loss
            loss = cfg.loss.id_loss_weight * id_loss + cfg.loss.triplet_loss_weight * triplet_loss

        # Backward pass
        optimizer.zero_grad()
        if cfg.training.use_amp:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()

        # Compute accuracy
        preds = torch.argmax(cls_score, dim=1)
        acc = (preds == pids).float().mean()

        # Reduce metrics across GPUs
        if get_world_size() > 1:
            loss = reduce_tensor(loss)
            id_loss = reduce_tensor(id_loss)
            triplet_loss = reduce_tensor(triplet_loss)
            acc = reduce_tensor(acc)

        # Update meters
        batch_size = imgs.size(0)
        meters.update({
            'loss': loss.item(),
            'id_loss': id_loss.item(),
            'triplet_loss': triplet_loss.item(),
            'acc': acc.item(),
            'triplet_prec': triplet_prec.item(),
        }, batch_size)

        batch_time.update(time.time() - end)
        end = time.time()

        # Log progress
        if is_main_process() and (batch_idx + 1) % cfg.training.log_period == 0:
            lr = optimizer.param_groups[0]['lr']
            logger.info(
                f'Epoch [{epoch}/{cfg.training.max_epochs}] '
                f'Batch [{batch_idx + 1}/{num_batches}] '
                f'Loss: {meters.get_avg("loss"):.4f} '
                f'ID: {meters.get_avg("id_loss"):.4f} '
                f'Tri: {meters.get_avg("triplet_loss"):.4f} '
                f'Acc: {meters.get_avg("acc"):.4f} '
                f'LR: {lr:.6f}'
            )

            # Log step metrics to WandB
            if cfg.logging.use_wandb:
                import wandb
                wandb.log({
                    'step': global_step,
                    'step/loss': loss.item(),
                    'step/id_loss': id_loss.item(),
                    'step/triplet_loss': triplet_loss.item(),
                    'step/acc': acc.item(),
                    'step/triplet_prec': triplet_prec.item(),
                    'step/lr': lr,
                }, step=global_step)

    # Step scheduler
    scheduler.step()

    return meters, global_step


@torch.no_grad()
def evaluate_model(model, val_loader, num_query, cfg, logger):
    """Evaluate model on validation set"""
    model.eval()

    feats_list = []
    pids_list = []
    camids_list = []

    for batch in val_loader:
        imgs, pids, camids, _, _ = batch
        imgs = imgs.cuda(non_blocking=True)

        # Extract features
        feat, _ = model(imgs)
        feats_list.append(feat.cpu())
        pids_list.append(pids)
        camids_list.append(camids)

    # Concatenate all features
    feats = torch.cat(feats_list, dim=0)
    pids = torch.cat(pids_list, dim=0).numpy()
    camids = torch.cat(camids_list, dim=0).numpy()

    # Gather from all GPUs
    if get_world_size() > 1:
        feats = all_gather_tensor(feats.cuda()).cpu()
        # Note: pids and camids need special handling for all_gather
        # For simplicity, only evaluate on rank 0 with gathered features

    if is_main_process():
        # Split query and gallery
        query_feat = feats[:num_query]
        gallery_feat = feats[num_query:]
        query_pids = pids[:num_query]
        gallery_pids = pids[num_query:]
        query_camids = camids[:num_query]
        gallery_camids = camids[num_query:]

        # Evaluate
        results = evaluate(
            query_feat, gallery_feat,
            query_pids, gallery_pids,
            query_camids, gallery_camids,
            feat_norm=(cfg.model.feat_norm == 'yes')
        )

        logger.info(
            f"Evaluation Results: "
            f"mAP: {results['mAP']:.4f} "
            f"Rank-1: {results['rank1']:.4f} "
            f"Rank-5: {results['rank5']:.4f} "
            f"Rank-10: {results['rank10']:.4f}"
        )

        return results

    return None


def save_checkpoint(state, is_best, output_dir, filename='checkpoint.pth'):
    """Save checkpoint"""
    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, filename)
    torch.save(state, filepath)
    if is_best:
        best_path = os.path.join(output_dir, 'best_model.pth')
        torch.save(state, best_path)


def main():
    parser = argparse.ArgumentParser(description='SOLIDER-REID DDP Training')
    parser.add_argument('--config', type=str, required=True, help='Path to config file')
    parser.add_argument('--resume', type=str, default=None, help='Path to checkpoint to resume')
    args = parser.parse_args()

    # Load config
    cfg = load_config(args.config)

    # Initialize distributed training
    distributed = init_distributed()

    # Set device
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    device = torch.device(f'cuda:{local_rank}')
    torch.cuda.set_device(device)

    # Set seed
    torch.manual_seed(cfg.training.seed + get_rank())
    np.random.seed(cfg.training.seed + get_rank())

    # Setup output directory
    output_dir = cfg.output_dir
    os.makedirs(output_dir, exist_ok=True)

    # Setup logger
    logger = setup_logger('reid', output_dir, get_rank())

    if is_main_process():
        logger.info("=" * 80)
        logger.info("SOLIDER-REID DDP Training")
        logger.info("=" * 80)
        logger.info(f"Distributed: {distributed}, World size: {get_world_size()}")
        logger.info(f"Config: {args.config}")
        logger.info(f"Output: {output_dir}")

    # Build data loaders
    train_loader, val_loader, num_classes, num_query = build_dataloader(cfg, is_train=True)

    if is_main_process():
        logger.info(f"Dataset: {cfg.data.dataset}")
        logger.info(f"Num classes: {num_classes}")
        logger.info(f"Num query: {num_query}")
        logger.info(f"Train batches: {len(train_loader)}")

    # Update num_classes in config
    cfg.model.num_classes = num_classes

    # Build model
    model = ReIDModel(
        model_name=cfg.model.name,
        num_classes=num_classes,
        pretrain_path=cfg.model.pretrain_path,
        semantic_weight=cfg.model.semantic_weight,
        img_size=tuple(cfg.data.img_size_train),
        drop_path_rate=cfg.model.drop_path_rate,
        drop_rate=cfg.model.drop_rate,
        attn_drop_rate=cfg.model.attn_drop_rate,
        neck=cfg.model.neck,
        neck_feat=cfg.model.neck_feat,
    )
    model = model.to(device)

    # Convert BatchNorm to SyncBatchNorm for DDP
    if distributed:
        model = nn.SyncBatchNorm.convert_sync_batchnorm(model)
        model = DDP(
            model,
            device_ids=[local_rank],
            output_device=local_rank,
            find_unused_parameters=True  # For SOLIDER's conditional branches
        )

    if is_main_process():
        logger.info(f"Model: {cfg.model.name}")
        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f"Trainable parameters: {num_params:,}")

    # Build optimizer and scheduler
    optimizer = build_optimizer(cfg, model)
    scheduler = build_scheduler(cfg, optimizer)

    # Build loss functions
    if cfg.loss.label_smoothing > 0:
        ce_loss_fn = LabelSmoothingCrossEntropy(epsilon=cfg.loss.label_smoothing)
    else:
        ce_loss_fn = nn.CrossEntropyLoss()
    triplet_loss_fn = TripletLoss(margin=cfg.loss.triplet_margin)

    # Mixed precision scaler
    scaler = GradScaler(enabled=cfg.training.use_amp)

    # Resume from checkpoint
    start_epoch = 0
    best_mAP = 0.0

    if args.resume:
        if os.path.isfile(args.resume):
            logger.info(f"Loading checkpoint: {args.resume}")
            checkpoint = torch.load(args.resume, map_location=device)
            start_epoch = checkpoint['epoch']
            best_mAP = checkpoint.get('best_mAP', 0.0)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            scheduler.load_state_dict(checkpoint['scheduler'])
            logger.info(f"Resumed from epoch {start_epoch}, best mAP: {best_mAP:.4f}")

    # WandB logging (only on main process)
    if is_main_process() and cfg.logging.use_wandb:
        import wandb
        wandb.init(
            project=cfg.logging.wandb_project,
            name=cfg.logging.experiment_name,
            config=vars(cfg),
            dir=output_dir,
        )

    # Training loop
    logger.info("Starting training...")
    global_step = start_epoch * len(train_loader)

    for epoch in range(start_epoch + 1, cfg.training.max_epochs + 1):
        # Train
        meters, global_step = train_one_epoch(
            model, train_loader, optimizer, scheduler,
            ce_loss_fn, triplet_loss_fn, scaler, cfg, epoch, logger, global_step
        )

        # Log epoch metrics to WandB
        if is_main_process() and cfg.logging.use_wandb:
            import wandb
            wandb.log({
                'epoch': epoch,
                'epoch/loss': meters.get_avg('loss'),
                'epoch/id_loss': meters.get_avg('id_loss'),
                'epoch/triplet_loss': meters.get_avg('triplet_loss'),
                'epoch/acc': meters.get_avg('acc'),
                'epoch/lr': optimizer.param_groups[0]['lr'],
            }, step=global_step)

        # Evaluate
        if epoch % cfg.training.eval_period == 0:
            results = evaluate_model(model, val_loader, num_query, cfg, logger)

            if is_main_process() and results:
                is_best = results['mAP'] > best_mAP
                best_mAP = max(results['mAP'], best_mAP)

                # Save checkpoint
                state = {
                    'epoch': epoch,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'scheduler': scheduler.state_dict(),
                    'best_mAP': best_mAP,
                    'config': vars(cfg),
                }
                save_checkpoint(state, is_best, output_dir, f'checkpoint_epoch_{epoch}.pth')

                if is_best:
                    logger.info(f"New best mAP: {best_mAP:.4f}")

                # Log to WandB
                if cfg.logging.use_wandb:
                    import wandb
                    wandb.log({
                        'val/mAP': results['mAP'],
                        'val/rank1': results['rank1'],
                        'val/rank5': results['rank5'],
                        'val/rank10': results['rank10'],
                        'val/best_mAP': best_mAP,
                    }, step=global_step)

        # Synchronize
        if distributed:
            dist.barrier()

    # Final evaluation
    if is_main_process():
        logger.info("=" * 80)
        logger.info("Training completed!")
        logger.info(f"Best mAP: {best_mAP:.4f}")
        logger.info(f"Checkpoints saved to: {output_dir}")
        logger.info("=" * 80)

    # Cleanup
    if cfg.logging.use_wandb and is_main_process():
        import wandb
        wandb.finish()

    cleanup()


if __name__ == '__main__':
    main()
